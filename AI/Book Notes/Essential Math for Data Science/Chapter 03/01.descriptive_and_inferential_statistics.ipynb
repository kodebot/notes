{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive and Inferential Statistics\n",
    "\n",
    "* statistics - practice of collecting and analysing data to discover findings that are useful or predict what causes those findings\n",
    "\n",
    "* machine learning in itself is a statistical tool\n",
    "\n",
    "* lot of blind sides in statistics, even for experienced professional - forgetting to see where the data comes from\n",
    "\n",
    "  * this problem gets even more significance as are automating most of the statistical algorithms\n",
    "\n",
    "* good to have solid understanding of statistics and hypothesis testing to avoid treating statistical automation as black box\n",
    "\n",
    "\n",
    "## What is Data?\n",
    "\n",
    "* data provides snapshots of a story\n",
    "\n",
    "  * may be biased\n",
    "\n",
    "  * may have gaps\n",
    "\n",
    "  * may be missing relevant data\n",
    "\n",
    "* data itself is not important but analysis and how it is collected is\n",
    "\n",
    "* not just source of truth, also source of AI\n",
    "\n",
    "* process of collecting data focused on a particular objective is *data mining*\n",
    "\n",
    "* data provides clues, not truth\n",
    "\n",
    "  * clues may lead to truth or erroneous conclusion\n",
    "\n",
    "* should be curious on where the data comes from\n",
    "\n",
    "  * garbage in garbage out\n",
    "\n",
    "### Ground Truth\n",
    "\n",
    "* correct/factual or true answer to a specific problem (rather than the one obtained by agent/sensor)\n",
    "\n",
    "* if an self-driven car fails to see a pedestrian on the road through its camera, can it not detect the failure and stop?\n",
    "\n",
    "  * no it can't as the system has no access ground truth. So no ground truth for it to fallback unless some other system or human provides it\n",
    "\n",
    "## Descriptive vs. Inferential Statistics\n",
    "\n",
    "* *descriptive statistics* summarises/describes data\n",
    "\n",
    "* *inferential statistics* tries to uncover attribute about larger population with a smaller sample\n",
    "\n",
    "  * could be wrong as our sample may not be representative of the population\n",
    "\n",
    "## Populations, Samples and Bias\n",
    "\n",
    "* a *population* is a particular group of interest we want to study\n",
    "\n",
    "  * example - all golden retrievers in Scotland\n",
    "\n",
    "  * does not have to tangible - it can be abstract too\n",
    "\n",
    "    * example - we want to study all the fights taking off between 2am and 3am\n",
    "\n",
    "      * not enough data for that time so our population is very small\n",
    "\n",
    "      * now we treat that population as sample - sample of all theoretical flights between 2am and 3am \n",
    "\n",
    "      * theoretical fights are abstract population\n",
    "\n",
    "* a *sample* is subset of a population that we are interested in\n",
    "\n",
    "  * usually random and unbiased\n",
    "\n",
    "  * sample must be as random as possible to avoid skewed conclusion\n",
    "\n",
    "* bias\n",
    "\n",
    "  * inevitable that our data will be biased\n",
    "\n",
    "    * so many `confounding variables` and factors\n",
    "\n",
    "      * confounding variable - unmeasured third variable that influences\n",
    "    \n",
    "    * only way to overcome this is by being truly random\n",
    "\n",
    "### A Whirlwind tour of Bias Types\n",
    "\n",
    "* geographical bias\n",
    "\n",
    "* confirmation bias\n",
    "\n",
    "  * collecting only data that supports your belief (knowingly or unknowingly)\n",
    "\n",
    "* self-selection bias\n",
    "\n",
    "  * a specific group more likely include themselves in a sample\n",
    "\n",
    "  * example 1 - conducting a poll on social media to find Netflix users - because they have internet so may be using Netflix more than non-social media users so it is not representative\n",
    "\n",
    "  * example 2 - polling customers in the flight whether they like the airline over the other airlines - the customers are self-selected as they already chose this airline to fly \n",
    "\n",
    "* survival bias\n",
    "\n",
    "  * captures only living and survived subjects\n",
    "\n",
    "  * examples are diverse but not obvious\n",
    "\n",
    "    * WWII - fighter jet armour - people were looking at returned flights to understand where the bullets were hit so the bullet hit areas can be armoured. But mathematician Abraham Wald pointed out that look at where the bullets are not hit. The flights did not return because they were hit in the areas where survived flights were not hit \n",
    "\n",
    "    * management consulting companies only looking at successful companies and using it as predictor for future success\n",
    "\n",
    "      * <https://xkcd.com/1827/>\n",
    "\n",
    "    * veterinary study of cats falling from 6 stories are less inflict great injury\n",
    "\n",
    "      * theory was that more than 6 story means the cats had enough time to brace for impact\n",
    "\n",
    "      * but later it was discovered that dead cats are not considered (more cats died falling from more than 6 stories)\n",
    "      \n",
    "        * who brings dead cat to veterinarian\n",
    "\n",
    "### Sample and Bias in Machine Learning\n",
    "\n",
    "* math and computers cannot detect bias in the data. It is on you as good Data Science professional to detect\n",
    "\n",
    "  * always scrutinise the data\n",
    "\n",
    "* causes Machine Learning algorithm to make biased conclusion\n",
    "\n",
    "  * Criminal Justice is one example - due to minority heavy dataset, the results are biased\n",
    "\n",
    "  * Volvo self-driving car test in 2017, could not recognise Kangaroo in Aus as the data trained on only had Deer and likes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Descriptive Statistics\n",
    "\n",
    "  ### Mean\n",
    "\n",
    "  * *mean* is average of set of values\n",
    "\n",
    "  * shows `center of gravity`\n",
    "\n",
    "  * $\\bar{x}$ is sample mean\n",
    "\n",
    "    * $\\bar{x} = \\sum \\frac{x_i}{n} $\n",
    "\n",
    "  * $\\mu$ is population mean\n",
    "\n",
    "    * $\\mu = \\sum \\frac{x_i}{N} $\n",
    "\n",
    "### Weighted Mean\n",
    "\n",
    "* is similar to *mean* but mean give equal importance to each item\n",
    "\n",
    "* weighted mean uses different weight for each item\n",
    "\n",
    "* calc\n",
    "\n",
    "  * $ \\frac{(x_1.w_1) + (x_2.w_2) + ... (x_n.w_n)}{w_1+w_2+...+w_n}$\n",
    "\n",
    "* why?\n",
    "\n",
    "  * one good example is to use weighted score from exams to give grade\n",
    "\n",
    "    * of 3 exams, give 20% weights to first 2 and 60% to the last\n",
    "\n",
    "### Median\n",
    "\n",
    "* middle most value of ordered set of values\n",
    "\n",
    "* if even number of values found, then median is average the the 2 middle most values\n",
    "\n",
    "* useful compared to mean when the data is skewed by outliers\n",
    "\n",
    "* example, in one University the average salary for Geography graduates is $250k but in other Unis it is just $22k\n",
    "\n",
    "  * turns out Michael Jordan, the basketball player is Geography graduate from that Uni\n",
    "\n",
    "* when data has many outliers then median is better as it is less sensitive to outliers\n",
    "\n",
    "* when mean and median are very different then the data is skewed by the outliers\n",
    "\n",
    "* median is `50% quantile` where 50% of the data is less than or equal to it\n",
    "\n",
    "* there are 25%, 50% and 75% quantiles\n",
    "\n",
    "  * these are referred to as `quartiles`\n",
    "\n",
    "    * 1st, 2nd and 3rd quartiles respectively\n",
    "\n",
    "### Mode\n",
    "\n",
    "* mode is most repetitive data\n",
    "\n",
    "* if no repetition then no mode\n",
    "\n",
    "* if two item occurs with same number of frequency then the data is `bimodal`\n",
    "\n",
    "* not used a lot in practice unless data is highly repetitive\n",
    "\n",
    "### Variance and Standard Deviation\n",
    "\n",
    "#### Population Variance and Standard Deviation\n",
    "\n",
    "* measures how spread out the data is\n",
    "\n",
    "* example: pets owned by colleagues (population)\n",
    "\n",
    "  * mean $\\mu = 6.5$\n",
    "\n",
    "  * we want to see how different each value is from mean\n",
    "\n",
    "    * so we subtract mean from each value\n",
    "\n",
    "    * we will have some positive and negative values\n",
    "\n",
    "    * and we can some what see how spread out each value is\n",
    "\n",
    "  * is there a way to summarise this with one number?\n",
    "\n",
    "    * we can find average of each of the differences \n",
    "\n",
    "      * but there are negative and positive numbers so they may cancel each other out and won't give real picture\n",
    "    \n",
    "    * we can use absolute values but even better approach is to square the differences\n",
    "\n",
    "      * main reason to square is - it amplifies larger differences\n",
    "\n",
    "      * also absolute values are not easy to work with in derivatives - NOT SURE WHY?\n",
    "\n",
    "    * so we find the average of squared differences and this is called `variance`\n",
    "\n",
    "    * $ population \\space variance = \\frac{(x_1 - mean)^2 ... (x_n - mean)^2}{N}$\n",
    "\n",
    "    * $ \\sigma^2 = \\frac{\\sum(x_i - \\mu)^2}{N}$\n",
    "\n",
    "* variance is great we can see how spread out the data is in one number but it is not in the same unit of the data\n",
    "\n",
    "* we need to change variance to be in the same unit as data, so we can compare it with the data directly\n",
    "\n",
    "  * so we find square root of variance - which is standard deviation\n",
    "\n",
    "  * $ \\sigma = \\sqrt{\\frac{\\sum(x_i - \\mu)^2}{N}}$\n",
    "\n",
    "#### Sample Variance and Standard Deviation\n",
    "\n",
    "* we need to make one important tweak to our population formulae to make it work for sample\n",
    "\n",
    "* need to reduce the total items by 1 when finding average of mean differences\n",
    "\n",
    "* this is to reduce the bias in the sample data and underestimate population variance\n",
    "\n",
    "  * this is because there is a good chance that the sample data is biased\n",
    "\n",
    "* by reducing the total by 1, we increase the variance there by standard deviation \n",
    "\n",
    "  * higher variance/standard deviation show greater uncertainty in our sample\n",
    "\n",
    "* sample variance\n",
    "\n",
    "  * $ s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}$\n",
    "\n",
    "* sample standard deviation\n",
    "\n",
    "  * $ s = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{n-1}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Distribution\n",
    "\n",
    "* normal distribution is most famous of all\n",
    "\n",
    "* also known as Gaussian distribution\n",
    "\n",
    "* bell shaped curve\n",
    "\n",
    "* most of the mass is around the mean\n",
    "\n",
    "* spread is defined in standard deviation\n",
    "\n",
    "* gets thinner and thinner as we move along either side of the tails\n",
    "\n",
    "![Image Normal Distribution](img/01.descriptive_and_inferential_statistics-1604081046.png)\n",
    "\n",
    "#### Discovering the Normal Distribution\n",
    "\n",
    "* we can use histogram to discover normal distribution\n",
    "\n",
    "* try adjusting bin sizes to discover bell shape\n",
    "\n",
    "* then scale it so that the area of the curve is 1.0\n",
    "\n",
    "  * this is needed for probability distribution\n",
    "\n",
    "#### Properties of a Normal Distribution\n",
    "\n",
    "* symmetrical\n",
    "\n",
    "* most mass centred around mean\n",
    "\n",
    "* spread is measured in standard deviation\n",
    "\n",
    "* tails are least likely, approaches zero but never reaches it\n",
    "\n",
    "* resembles lot of phenomena in nature\n",
    "\n",
    "#### Probability Density Function (PDF)\n",
    "\n",
    "* for normal distribution\n",
    "\n",
    "* $ f(x) = \\frac{1}{\\sigma} \\times \\sqrt{2\\pi} \\times e^{-\\frac{1}{2}(\\frac{x-\\mu^2}{\\sigma})}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def normal_pdf(x: float, mean: float, std_dev: float) -> float:\n",
    "    return (1.0 / (2.0 * math.pi * std_dev ** 2) ** 0.5) * math.exp(-1.0 * ((x - mean) ** 2 / (2.0 * std_dev ** 2)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Cumulative Distribution Function (CDF)\n",
    "\n",
    "* the vertical axis in the PDF is not the probability but likelihood\n",
    "\n",
    "* to find probability we need to find area under the curve for the input range\n",
    "\n",
    "  * i.e. probability of age range between 55 and 60\n",
    "\n",
    "* CDF provides area up to that point\n",
    "\n",
    "* we can find probability using deductive approach from CDF\n",
    "\n",
    "  * for example to find probability between 55 and 60\n",
    "\n",
    "  * we can find CDF up to 60 and CDF up to 55 then subtract CDF(60)\n",
    "\n",
    "  ![](img/01.descriptive_and_inferential_statistics-1604090739.png)\n",
    "\n",
    "* CDF shows S shaped curve called sigmoid curve\n",
    "\n",
    "![](img/01.descriptive_and_inferential_statistics-1604090845.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4920450147062894"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "mean = 64.43\n",
    "std_dev = 2.99\n",
    "\n",
    "x = norm.cdf(66, mean, std_dev) - norm.cdf(62, mean, std_dev)\n",
    "x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse CDF\n",
    "\n",
    "* used to look up x-value from the probability (opposite of CDF)\n",
    "\n",
    "* `norm.ppf` function in scipy is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.3481123445849\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "x = norm.ppf(.95, loc=64.43, scale=2.99)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z-Scores\n",
    "\n",
    "* rescale normal distribution such that\n",
    "\n",
    "  * mean is 0\n",
    "\n",
    "  * standard deviation is 1\n",
    "\n",
    "  * this is called `standard normal distribution`\n",
    "\n",
    "* standard normal distribution is useful to compare two different normal distributions\n",
    "\n",
    "* z-score is x-value represented in terms of standard deviation\n",
    "\n",
    "  * it indicates how many standard deviations away an x value is from the mean\n",
    "\n",
    "  * $ z = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "* example\n",
    "\n",
    "  * comparing the two house prices from two different neighbourhood and find which one is expensive relative to their neighbourhood\n",
    "\n",
    "  * we calculate z-score for each of the house and the bigger the z-score means expensive house\n",
    "\n",
    "### Coefficient of Variation\n",
    "\n",
    "* useful to compare the spread of two different distributions\n",
    "\n",
    "* $cv = \\frac{\\sigma}{\\mu}$\n",
    "\n",
    "* higher value means higher spread\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
