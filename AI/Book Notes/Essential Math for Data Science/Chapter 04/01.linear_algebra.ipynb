{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "* misconception that Linear algebra is algebra and that is all about plotting line with $y = mx + b $\n",
    "\n",
    "* this is why it should be referred to as `Vector Algebra` or `Matrix Algebra` as it is much more to do with than just plotting lines\n",
    "\n",
    "  * much more useful is many metaphysical ways\n",
    "\n",
    "* concerns itself with linear systems but represents them with `vectors` and `matrices` \n",
    "\n",
    "* inevitable to learn Linear Algebra in ML and Data Science to see what is going on inside the black box\n",
    "\n",
    "## What is a Vector?\n",
    "\n",
    "* imagine vector as arrow in the space with specific length and direction, its tail sits at (0, 0) of Cartesian plane\n",
    "\n",
    "  * vector usually represents data\n",
    "\n",
    "  * usually represents as $ \\bar{v} = \\begin{bmatrix}x \\\\ y \\end{bmatrix} $\n",
    "\n",
    "  * example, \n",
    "\n",
    "    * $\\bar{v} = \\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$\n",
    "\n",
    "      * think of this as 3 to the right and 2 to the top\n",
    "\n",
    "        * ![](img/01.linear_algebra-2604064652.png)\n",
    "  \n",
    "    * house with 18000 square feet and $260000 price can be represented as\n",
    "\n",
    "      * $\\bar{v} = \\begin{bmatrix} 18000 \\\\ 260000 \\end{bmatrix}$\n",
    "\n",
    "* use NumPy array or SciPy to represent and perform operations on vector rather than Python list\n",
    "\n",
    "  * for computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([3, 2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vector\n",
    "\n",
    "  * in physics\n",
    "\n",
    "    * direction and magnitude\n",
    "\n",
    "  * in math\n",
    "\n",
    "    * direction and scale on XY plane\n",
    "\n",
    "  * in computer science\n",
    "\n",
    "    * way of storing list of numbers\n",
    "\n",
    "* computer science view is the one that will be used for DS and ML\n",
    "\n",
    "* without visual representation of what a vector is\n",
    "\n",
    "  * it is impossible to grasp may fundamental concepts like `linear dependence`, `determinant`, etc...\n",
    "\n",
    "  ![](img/01.linear_algebra-2604065818.png)\n",
    "\n",
    "\n",
    "* where is it used\n",
    "\n",
    "  * computer graphics (animations and transformations)\n",
    "\n",
    "  * linear programming (linear algebra is used to maximise/minimise the solutions within the given constraints)\n",
    "\n",
    "  * physical and physics related simulations\n",
    "\n",
    "  * used is so many areas so it is hard to see the generalisation\n",
    "\n",
    "* dimension\n",
    "\n",
    "  * vector can be on n dimension, but it is hard to visualise beyond 3d\n",
    "\n",
    "\n",
    "### Adding and Combining Vectors\n",
    "\n",
    "* moving tail of one to the tip of another and walking to end of the tip\n",
    "\n",
    "![](img/01.linear_algebra-2604072254.png)\n",
    "\n",
    "![](img/01.linear_algebra-2704060019.png)\n",
    "\n",
    "* numerically just adding corresponding elements\n",
    "\n",
    "* vector addition is `commutative`\n",
    "\n",
    "  * $ \\bar{a} + \\bar{b} = \\bar{b} + \\bar{a} $\n",
    "\n",
    "\n",
    "### Scaling Vectors\n",
    "\n",
    "* scaling is growing or shrinking the length of the vector by the scaling factor called `scalar` \n",
    "\n",
    "* mathematically, multiple each element with the scalar \n",
    "\n",
    "![](img/01.linear_algebra-2704060756.png)\n",
    "\n",
    "* scaling does not change the direction, just the magnitude\n",
    "\n",
    "  * negative number flips the direction\n",
    "\n",
    "  * but it still stays in the same line (linearly dependent)\n",
    "\n",
    "  ![](img/01.linear_algebra-2704061859.png)\n",
    "\n",
    "### Span and Linear Dependence\n",
    "\n",
    "* with linear combination (scaling and adding two vectors together) we can reach unlimited number of vectors\n",
    "\n",
    "  * this is called the `span` of those two vectors\n",
    "\n",
    "  * span will be unlimited if the vectors are pointing in different directions (not in same line)\n",
    "\n",
    "    * linear independence\n",
    "\n",
    "  * span will be limited if they are in the same line\n",
    "\n",
    "    * linear dependence\n",
    "\n",
    "    * for 3 or more dimensions, the span would be limited to a plane in the lower dimension\n",
    "\n",
    "* `determinant` helps to find if a vectors are linearly independent or not\n",
    "\n",
    "* it is useful to know because solving system of linear equations becomes difficult or unsolvable if they are linearly dependent\n",
    "\n",
    "\n",
    "## Linear Transformation\n",
    "\n",
    "* use vector to transform another vector in a function-like manner\n",
    "\n",
    "### Basis Vectors\n",
    "\n",
    "* $\\hat{i}, \\hat{j}$ are basis vectors used to describe transformations on other vectors\n",
    "\n",
    "* usually perpendicular to each other in positive direction with length 1\n",
    "\n",
    "![](img/01.linear_algebra-2704070230.png)\n",
    "\n",
    "* basis to build or form any vector (using linearly independence and span)\n",
    "\n",
    "* matrix is collection of vectors with `n` rows and `m` columns, it convenient way to package data\n",
    "\n",
    "* basis vector can be combined into matrix form\n",
    "\n",
    "  * $\\hat{i} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "  * $\\hat{j} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "  * $basis = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "![](img/01.linear_algebra-2704071044.png)\n",
    "\n",
    "* linear transformation\n",
    "\n",
    "  * stretch, squish, sheer or rotate vector by tracking its basis vector movements\n",
    "\n",
    "  ![](img/01.linear_algebra-2704071348.png)\n",
    "\n",
    "* with linear transformation, we can achieve\n",
    "\n",
    "  * scale\n",
    "\n",
    "  * rotation\n",
    "\n",
    "  * sheer\n",
    "\n",
    "  * inversion\n",
    "\n",
    "  ![](img/01.linear_algebra-2704071641.png)\n",
    "\n",
    "* we cannot have transformation that results in curvy or squiggly, hence `linear algebra`\n",
    "\n",
    "### Matrix Vector Multiplication\n",
    "\n",
    "* tracking where $\\hat{i}$ and $\\hat{j}$ lands after transformation helps to see where a new vector lands\n",
    "\n",
    "* $\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix}$\n",
    "\n",
    "* $\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = \\begin{bmatrix} x.a + y.b \\\\ x.c + y.d \\end{bmatrix}$\n",
    "\n",
    "* this formula may seem confusing but it is a packed notation of scaling basis vectors and adding them together\n",
    "\n",
    "* $\\begin{bmatrix} x_{new} \\\\ y_{new} \\end{bmatrix} = x.\\begin{bmatrix} a \\\\ c\\end{bmatrix} y.\\begin{bmatrix} b \\\\ d\\end{bmatrix}$\n",
    "\n",
    "* matrix is just a packed notation of basis vectors $\\hat{i}, \\hat{j}$\n",
    "\n",
    "* in NumPy we use `dot` method of an array to perform matrix vector multiplication\n",
    "\n",
    "* in NumPy each inner array represents row vector, so when composing matrix from $\\hat{i}, $\\hat{j}$, the matrix need to be transposed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "basis = np.array(\n",
    "    [[3, 0],\n",
    "     [0, 3]]\n",
    ")\n",
    "\n",
    "v = np.array([1, 1])\n",
    "\n",
    "basis.dot(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Declare i-hat and j-hat\n",
    "i_hat = np.array([2, 0])\n",
    "j_hat = np.array([0, 3])\n",
    "\n",
    "# compose basis matrix using i-hat and j-hat\n",
    "# also need to transpose rows into columns\n",
    "basis = np.array([i_hat, j_hat]).transpose()\n",
    "\n",
    "# declare vector v\n",
    "v = np.array([1,1])\n",
    "\n",
    "# create new vector\n",
    "# by transforming v with dot product\n",
    "basis.dot(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the transformation works the same way for 3 or more dimensions as well\n",
    "\n",
    "* some transformation can shift vector space into fewer or more dimensions (this is what a non-square matrix will do)\n",
    "\n",
    "## Matrix Multiplication\n",
    "\n",
    "* matrix multiplication - applying multiple transformations to vector\n",
    "\n",
    "* $\\begin{bmatrix}a & b \\\\ c & d \\end{bmatrix}\\begin{bmatrix}e & f \\\\ g & h \\end{bmatrix} = \\begin{bmatrix}ae + bg & af + bh \\\\ ce + dy & cf+dh \\end{bmatrix}$\n",
    "\n",
    "* memory trick - `over and down`\n",
    "\n",
    "* `matmul` in numpy is used for matrix multiplication\n",
    "\n",
    "  * `@` shorthand\n",
    "\n",
    "* order of transformation matters\n",
    "\n",
    "  * matrix product is NOT commutative (i.e. cannot flip orders)\n",
    "\n",
    "  * always apply inner most to outer most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED MATRIX:\n",
      " [[ 1 -1]\n",
      " [ 1  0]]\n",
      "[-1  1]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "\n",
    "# Transformation 1\n",
    "i_hat1 = array([0, 1])\n",
    "j_hat1 = array([-1, 0])\n",
    "transform1 = array([i_hat1, j_hat1]).transpose()\n",
    "\n",
    "# Transformation 2\n",
    "i_hat2 = array([1, 0])\n",
    "j_hat2 = array([1, 1])\n",
    "transform2 = array([i_hat2, j_hat2]).transpose()\n",
    "\n",
    "# Combine Transformations\n",
    "combined = transform2 @ transform1\n",
    "\n",
    "# Test\n",
    "print(\"COMBINED MATRIX:\\n {}\".format(combined))\n",
    "v = array([1, 2])\n",
    "print(combined.dot(v)) # [-1, 1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinants\n",
    "\n",
    "* tells how much a vector space changes in scale with linear transformation\n",
    "\n",
    "  * the factor by which it changes is called `determinant`\n",
    "\n",
    "  * i.e it tells how much vector space is stretched or squished\n",
    "\n",
    "  ![](img/01.linear_algebra-0405063950.png)\n",
    "\n",
    "* factor of 6 means - vector space scaled up by 6 times\n",
    "\n",
    "* factor of 1 means - no scaling to vector space\n",
    "\n",
    "  * rotating and shearing will have determinant of 1\n",
    "\n",
    "  ![](img/01.linear_algebra-0405064225.png)\n",
    "\n",
    "* when orientation is flipped ($\\hat{i}, \\hat{j}$, changes position clockwise) \n",
    "\n",
    "  * the determinant will be negative\n",
    "\n",
    "  ![](img/01.linear_algebra-0405064455.png)\n",
    "\n",
    "* if determinant is 0, then it means we have `linearly dependent` vectors\n",
    "\n",
    "  * vector space is squished into lesser dimensions by the transformation\n",
    "\n",
    "  * this means we have harder problem at hand to solve\n",
    "\n",
    "  ![](img/01.linear_algebra-0405064724.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n",
      "1.0\n",
      "-5.000000000000001\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import det\n",
    "from numpy import array\n",
    "\n",
    "# Calculate determinant\n",
    "i_hat = array([3, 0])\n",
    "j_hat = array([0, 2])\n",
    "basis = array([i_hat, j_hat]).transpose()\n",
    "determinant = det(basis)\n",
    "print(determinant) # prints 6.0\n",
    "\n",
    "# Determinant for Shear\n",
    "i_hat = array([1, 0])\n",
    "j_hat = array([1, 1])\n",
    "basis = array([i_hat, j_hat]).transpose()\n",
    "determinant = det(basis)\n",
    "print(determinant) # prints 1.0\n",
    "\n",
    "# Negative determinant\n",
    "i_hat = array([-2, 1])\n",
    "j_hat = array([1, 2])\n",
    "basis = array([i_hat, j_hat]).transpose()\n",
    "determinant = det(basis)\n",
    "print(determinant) # prints -5.0\n",
    "\n",
    "# Zero determinant\n",
    "i_hat = array([-2, 1])\n",
    "j_hat = array([3, -1.5])\n",
    "basis = array([i_hat, j_hat]).transpose()\n",
    "determinant = det(basis)\n",
    "print(determinant) # prints 0.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Types of Matrices\n",
    "\n",
    "### Square Matrix\n",
    "\n",
    "* equal number of rows and columns\n",
    "\n",
    "* $A = \\begin{bmatrix} 4 & 2 & 7 \\\\ 5 & 1 & 9 \\\\ 4 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "* mainly used to represent linear transformations\n",
    "\n",
    "* requirement for many operations like `eigendecomposition`\n",
    "\n",
    "### Identity Matrix\n",
    "\n",
    "* square matrix with 1s along the leading diagonal and 0s in other places\n",
    "\n",
    "* $I = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\n",
    "\n",
    "* when applied to another matrix, it leaves the original matrix\n",
    "\n",
    "  * useful when reversing a transformation\n",
    "\n",
    "### Inverse Matrix\n",
    "\n",
    "* undo the matrix transformation\n",
    "\n",
    "* inverse of $A$ is represented using $A^{-1}$\n",
    "\n",
    "* $ A = \\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix}$\n",
    "\n",
    "* $ A^{-1} = \\begin{bmatrix} -\\frac{1}{2} & 0 & \\frac{1}{3} \\\\ 5.5 & -2 & \\frac{4}{3} \\\\ -2 & 1 & \\frac{1}{3} \\end{bmatrix}$\n",
    "\n",
    "* $A A^{-1} = A^{-1} A = I$\n",
    "\n",
    "### Diagonal Matrix\n",
    "\n",
    "* non zero in leading diagonal and zeros in other places\n",
    "\n",
    "* $D = \\begin{bmatrix} 4 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}$\n",
    "\n",
    "* desirable in certain computation as it represents simple scaler applied to vector space\n",
    "\n",
    "### Triangular Matrix\n",
    "\n",
    "* non-zero value in leading diagonal and above (upper triangle) or below (lower triangle)\n",
    "\n",
    "* $ U = \\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 4 & 5 \\\\ 0 & 0 & 6 \\end{bmatrix}$\n",
    "\n",
    "* $ L = \\begin{bmatrix} 1 & 0 & 0 \\\\ 2 & 3 & 0 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n",
    "\n",
    "* used in numerical analysis as these are typically easier to solve\n",
    "\n",
    "* show up in decompositions like LU decompositions\n",
    "\n",
    "### Sparse Matrix\n",
    "\n",
    "* matrix with mostly zero and few non-zero values\n",
    "\n",
    "* not very interesting from pure mathematical point of view\n",
    "\n",
    "* but provides efficiency opportunity from computing point of view\n",
    "\n",
    "  * when storing we only need to track where non-zero values are ignoring zeros\n",
    "\n",
    "* $\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 2 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}$\n",
    "\n",
    "## Systems of Equations and Inverse Matrices\n",
    "\n",
    "* solving systems of linear equations is one of the use of linear algebra\n",
    "\n",
    "* good way to explain inverse matrix\n",
    "\n",
    "* if we want computer to solve the following equations efficiently, we need to represent them in the matrix form\n",
    "\n",
    "  * $4x+2y+4z=44$\n",
    "\n",
    "  * $5x+3y+7z=56$\n",
    "\n",
    "  * $9x+3y+6z=72$\n",
    "\n",
    "  * convert these into AX=B\n",
    "\n",
    "  * coefficient matrix $A= \\begin{bmatrix} 4 & 2 & 4 \\\\ 5 & 3 & 7 \\\\ 9 & 3 & 6 \\end{bmatrix}$\n",
    "\n",
    "  * unknowns vector $X=\\begin{bmatrix}x \\\\ y \\\\ z \\end{bmatrix}$\n",
    "\n",
    "  * results vector $B=\\begin{bmatrix}44 \\\\ 56 \\\\ 72 \\end{bmatrix}$\n",
    "\n",
    "  * now solving it is easy, we just to need to multiply pre-multiply both sides with $A^{-1}$\n",
    "\n",
    "  * $A^{-1}AX = A^{-1}B$\n",
    "\n",
    "  * $IX = A^{-1}B$, $A^{-1}A = I$\n",
    "\n",
    "  * use computer to find $A^{-1}$ rather than Gaussian elimination or other methods manually\n",
    "\n",
    "  * $ A^{-1} = \\begin{bmatrix} -\\frac{1}{2} & 0 & \\frac{1}{3} \\\\ 5.5 & -2 & \\frac{4}{3} \\\\ -2 & 1 & \\frac{1}{3} \\end{bmatrix}$\n",
    "\n",
    "  * NumPy will not make identity matrix obvious due to decimal precision but doing it symbolically using SymPy allow us to see them\n",
    "\n",
    "  * linear programming is using to maximise/minimise a result with defined inequality constraints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVERSE:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\frac{1}{2} & 0 & \\frac{1}{3}\\\\\\frac{11}{2} & -2 & - \\frac{4}{3}\\\\-2 & 1 & \\frac{1}{3}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-1/2,  0,  1/3],\n",
       "[11/2, -2, -4/3],\n",
       "[  -2,  1,  1/3]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDENTITY:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0\\\\0 & 1 & 0\\\\0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 0],\n",
       "[0, 1, 0],\n",
       "[0, 0, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inverse and Identity matrix using SymPy\n",
    "\n",
    "from sympy import *\n",
    "\n",
    "# y4x + 2y + 4z = 44\n",
    "# 5x + 3y + 7z = 56\n",
    "# 9x + 3y + 6z = 72\n",
    "A = Matrix([\n",
    "[4, 2, 4],\n",
    "[5, 3, 7],\n",
    "[9, 3, 6]\n",
    "])\n",
    "# dot product between A and its inverse\n",
    "# will produce identity function\n",
    "inverse = A.inv()\n",
    "identity = inverse * A\n",
    "# prints Matrix([[-1/2, 0, 1/3], [11/2, -2, -4/3], [-2, 1, 1/3]])\n",
    "print(\"INVERSE:\")\n",
    "display(inverse)\n",
    "# prints Matrix([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "print(\"IDENTITY:\")\n",
    "display(identity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2. 34. -8.]\n"
     ]
    }
   ],
   "source": [
    "# using NumPy to solve systems of linear equations\n",
    "\n",
    "from numpy import array\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# 4x + 2y + 4z = 44\n",
    "# 5x + 3y + 7z = 56\n",
    "# 9x + 3y + 6z = 72\n",
    "A = array([\n",
    "[4, 2, 4],\n",
    "[5, 3, 7],\n",
    "[9, 3, 6]\n",
    "])\n",
    "B = array([\n",
    "44,\n",
    "56,\n",
    "72\n",
    "])\n",
    "X = inv(A).dot(B)\n",
    "print(X) # [ 2. 34. -8.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2\\\\34\\\\-8\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 2],\n",
       "[34],\n",
       "[-8]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SymPy to solve systems of linear equations\n",
    "\n",
    "# 4x + 2y + 4z = 44\n",
    "# 5x + 3y + 7z = 56\n",
    "# 9x + 3y + 6z = 72\n",
    "A = Matrix([\n",
    "[4, 2, 4],\n",
    "[5, 3, 7],\n",
    "[9, 3, 6]\n",
    "])\n",
    "B = Matrix([\n",
    "44,\n",
    "56,\n",
    "72])\n",
    "X = A.inv() * B\n",
    "display(X) # Matrix([[2], [34], [-8]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors and Eigenvalues\n",
    "\n",
    "* matrix decomposition is factoring a matrix into multiple parts, just like factoring a number $12 = 2 \\times 6$\n",
    "\n",
    "* very useful for tasks like\n",
    "\n",
    "  * finding inverses\n",
    "\n",
    "  * finding determinant\n",
    "\n",
    "  * linear regression\n",
    "\n",
    "* there are many types of decomposition, for example QR decomposition is used in linear regression\n",
    "\n",
    "* `eigendecomposition` is another one and very useful for machine learning and `principal component analysis`\n",
    "\n",
    "  * only works with square matrix\n",
    "\n",
    "  * there are two parts \n",
    "\n",
    "    * eigenvalues ($\\lambda$)\n",
    "\n",
    "    * eigenvector ($v$)\n",
    "\n",
    "    * ![](img/01.linear_algebra-0505070722.png)\n",
    "\n",
    "  * for square matrix A\n",
    "\n",
    "    * $Av = \\lambda v$\n",
    "\n",
    "    * there is one eigenvalue and eigenvector for each dimension of parent matrix\n",
    "\n",
    "    * not all matrix can be composed of eigenvector and eigenvalue\n",
    "\n",
    "    * sometimes complex imaginary number will result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EIGENVALUES\n",
      "[-0.46410162  6.46410162]\n",
      "\n",
      "EIGENVECTORS\n",
      "[[-0.80689822 -0.34372377]\n",
      " [ 0.59069049 -0.9390708 ]]\n"
     ]
    }
   ],
   "source": [
    "# eigendemposition in NumPy\n",
    "\n",
    "from numpy import array, diag\n",
    "from numpy.linalg import eig, inv\n",
    "A = array([\n",
    "[1, 2],\n",
    "[4, 5]\n",
    "])\n",
    "\n",
    "eigenvals, eigenvecs = eig(A)\n",
    "print(\"EIGENVALUES\")\n",
    "print(eigenvals)\n",
    "print(\"\\nEIGENVECTORS\")\n",
    "print(eigenvecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
