{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "* people or animals learn by interacting with environment\n",
    "  * they acutely aware how environment responds to what they do\n",
    "  * they seek to influence what happens through their behaviour (action/interaction)\n",
    "\n",
    "* learning from interaction is foundational idea underlying all theories of learning and intelligence\n",
    "\n",
    "* the focus of this book is to explore computational approach to learning from interaction\n",
    "\n",
    "* the approach explored is called `reinforcement learning` which focused on goal-directed learning from interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Reinforcement Learning\n",
    "\n",
    "* is a learning of what to do\n",
    "\n",
    "* how to map situations (states) to actions - so as to maximise numerical reward signal\n",
    "\n",
    "* learner is not told which action to take, but instead discover which actions yield max reward by trying them\n",
    "\n",
    "* in most cases, action not only affect immediate reward but also next situation (state), thereby affecting future reward\n",
    "\n",
    "* two most important distinguishing features of RL are\n",
    "  * trial-and-error search\n",
    "  * delayed reward\n",
    "\n",
    "* problems of RL are formalised using ideas from dynamical systems theory - specifically\n",
    "  * as the optimal control of incompletely-known Markov Decision Process (MDP)\n",
    "\n",
    "* learning agent\n",
    "  * must be able to sense the state of its environment to some extend\n",
    "  * must be able to take actions that affect state\n",
    "  * must have goal(s) relating to state of the environment (for example reaching a particular state)\n",
    "\n",
    "* MDP does include state, action and goal in their simplest form without trivialising any of them\n",
    "\n",
    "* RL is considered ML paradigm of its own and different from other paradigms like supervised and unsupervised learning\n",
    "\n",
    "* exploration-exploitation trade off is common challenge in RL compared to other types of learning\n",
    "  * to obtain reward, RL agent must prefer actions that it has tried in the past and found to be effective (exploit)\n",
    "  * but to discover such actions, it has to try actions that it has not selected before (explore)\n",
    "  * on a stochastic task, each action must be tried multiple times to gain reliable estimate of expected reward\n",
    "  * even though this problem has been intensively studies for many decades, it remain still unsolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Elements of RL\n",
    "\n",
    "* elements\n",
    "  * agent\n",
    "  * environment\n",
    "  * policy\n",
    "  * reward signal\n",
    "  * value function\n",
    "  * model of the environment (optional)\n",
    "\n",
    "* **Policy**\n",
    "  * the learning agent's way of behaving at a given time\n",
    "  * mapping between perceived state of env to actions to be taken\n",
    "  * policy can be simple lookup table or something that involve complex computation\n",
    "  * in general, policy is stochastic, specifying probabilities of each actions (at a particular state)\n",
    "\n",
    "* **Reward Signal**\n",
    "  * defines goal of the RL problem\n",
    "  * at each step, env send a single number called `reward` and the goal of RL is to maximise the total reward\n",
    "\n",
    "* **Value function**\n",
    "  * defines what is good in the long run as opposed to Reward signal which indicates what is good in a immediate sense\n",
    "  * for example value function might says recharging batter is good rather than collecting immediate rewards all the time\n",
    "  * agent take actions to reach a state of highest value not the highest reward, eventually leading to highest overall rewards\n",
    "  * rewards are directly available from env, but value must be estimated and re-estimated from sequence of observations an agent make over entire lifetime\n",
    "  * much harder and most important component of RL is to find methods to efficiently estimate values\n",
    "\n",
    "* **Model**\n",
    "  * something that mimics the behaviour of the env\n",
    "  * i.e. something that allow inferences to be made about how evn will behave\n",
    "  * e.g. given current state and action, the model might predict next state and reward\n",
    "  * used for `planning` - meaning - a way of deciding on a course of action by considering possible future states before they are actually experienced\n",
    "  * RL methods that solves problems using model is called `model-based` as opposed to simpler `model-free`\n",
    "  * `model-free` uses trial-and-error - a complete opposite of planning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Limitations and Scopes\n",
    "\n",
    "* the focus of this book is \n",
    "  * decision-making issues - not how to design states of the environment\n",
    "  * algorithms that estimates value rather than `evolutionary methods` like genetic algorithm, genetic programming, etc...\n",
    "    * evolutionary methods \n",
    "      * work by selecting random policies and applying them to separate instances of the env\n",
    "      * polices that performed better taken to next generation of polices and the process repeats\n",
    "      * this is called evolutionary because it mimic biological evolution\n",
    "      * might be good choice when policy space is small or when complete state is not observable\n",
    "      * doesn't use interaction as a mechanism to learn\n",
    "      * in general, not suitable for RL, hence not covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Extended Example - Tic-Tac-Toe\n",
    "## 1.6. Summary\n",
    "## 1.7 Early History of RL\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
